{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "207265ae-1f97-41aa-b9fc-6549826c855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os \n",
    "content_dir = os.getcwd() + '/test_content/'\n",
    "style_dir = os.getcwd() + '/test_style/'\n",
    "model_dir = os.getcwd() + '/Models/'\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93bdd469-6c51-4e24-9673-e70103769011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vgg definition that conveniently let's you grab the outputs from any layer\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, pool='max'):\n",
    "        super(VGG, self).__init__()\n",
    "        #vgg modules\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        if pool == 'max':\n",
    "            self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        elif pool == 'avg':\n",
    "            self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool4 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool5 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "    def forward(self, x, out_keys):\n",
    "        out = {}\n",
    "        out['r11'] = F.relu(self.conv1_1(x))\n",
    "        out['r12'] = F.relu(self.conv1_2(out['r11']))\n",
    "        out['p1'] = self.pool1(out['r12'])\n",
    "        out['r21'] = F.relu(self.conv2_1(out['p1']))\n",
    "        out['r22'] = F.relu(self.conv2_2(out['r21']))\n",
    "        out['p2'] = self.pool2(out['r22'])\n",
    "        out['r31'] = F.relu(self.conv3_1(out['p2']))\n",
    "        out['r32'] = F.relu(self.conv3_2(out['r31']))\n",
    "        out['r33'] = F.relu(self.conv3_3(out['r32']))\n",
    "        out['r34'] = F.relu(self.conv3_4(out['r33']))\n",
    "        out['p3'] = self.pool3(out['r34'])\n",
    "        out['r41'] = F.relu(self.conv4_1(out['p3']))\n",
    "        out['r42'] = F.relu(self.conv4_2(out['r41']))\n",
    "        out['r43'] = F.relu(self.conv4_3(out['r42']))\n",
    "        out['r44'] = F.relu(self.conv4_4(out['r43']))\n",
    "        out['p4'] = self.pool4(out['r44'])\n",
    "        out['r51'] = F.relu(self.conv5_1(out['p4']))\n",
    "        out['r52'] = F.relu(self.conv5_2(out['r51']))\n",
    "        out['r53'] = F.relu(self.conv5_3(out['r52']))\n",
    "        out['r54'] = F.relu(self.conv5_4(out['r53']))\n",
    "        out['p5'] = self.pool5(out['r54'])\n",
    "        return [out[key] for key in out_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad6d5aa-30e4-4327-b934-5d5aea9025a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gram matrix and loss\n",
    "class GramMatrix(nn.Module):\n",
    "    def forward(self, input):\n",
    "        b,c,h,w = input.size()\n",
    "        F = input.view(b, c, h*w)\n",
    "        G = torch.bmm(F, F.transpose(1,2)) \n",
    "        G.div_(h*w)\n",
    "        return G\n",
    "\n",
    "class GramMSELoss(nn.Module):\n",
    "    def forward(self, input, target):\n",
    "        out = nn.MSELoss()(GramMatrix()(input), target)\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa94e988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "checkpoint = \"vinvino02/glpn-nyu\"\n",
    "depth_estimator = pipeline(\"depth-estimation\", model=checkpoint,device=device)\n",
    "\n",
    "def depth_loss(synthesis_image,content_image,depth_estimator):\n",
    "    if True:\n",
    "        tensor = synthesis_image\n",
    "        tensor = tensor.cpu().clone()\n",
    "        tensor = tensor.squeeze(0)\n",
    "        tensor = tensor.permute(1, 2, 0)\n",
    "        image = tensor.detach().numpy()\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "        synthesis_image = Image.fromarray(image)\n",
    "    image_size = synthesis_image.size\n",
    "    synthesis_image = synthesis_image.resize((image_size[0]//3,image_size[1]//3))\n",
    "    content_image = content_image.resize((image_size[0]//3,image_size[1]//3))\n",
    "    synthesis_depth = depth_estimator(synthesis_image)[\"depth\"]\n",
    "    synthesis_depth = torch.from_numpy(np.asarray(synthesis_depth).copy()).float()\n",
    "    synthesis_depth = (synthesis_depth-torch.min(synthesis_depth))/(torch.max(synthesis_depth)-torch.min(synthesis_depth))\n",
    "    content_depth = depth_estimator(content_image)[\"depth\"]\n",
    "    content_depth = torch.from_numpy(np.asarray(content_depth).copy()).float()\n",
    "    content_depth = (content_depth-torch.min(content_depth))/(torch.max(content_depth)-torch.min(content_depth))\n",
    "    return torch.sum(torch.abs(synthesis_depth-content_depth))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579d4321-dd9a-494b-8c39-64c94ea744a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre and post processing for images\n",
    "img_size = 512 \n",
    "prep = transforms.Compose([transforms.Resize(img_size),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to BGR\n",
    "                           transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], #subtract imagenet mean\n",
    "                                                std=[1,1,1]),\n",
    "                           transforms.Lambda(lambda x: x.mul_(255)),\n",
    "                          ])\n",
    "postpa = transforms.Compose([transforms.Lambda(lambda x: x.mul_(1./255)),\n",
    "                           transforms.Normalize(mean=[-0.40760392, -0.45795686, -0.48501961], #add imagenet mean\n",
    "                                                std=[1,1,1]),\n",
    "                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to RGB\n",
    "                           ])\n",
    "postpb = transforms.Compose([transforms.ToPILImage()])\n",
    "def postp(tensor): # to clip results in the range [0,1]\n",
    "    t = postpa(tensor)\n",
    "    t[t>1] = 1    \n",
    "    t[t<0] = 0\n",
    "    img = postpb(t)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2259fdc-5a94-4e38-8a3a-eb22a0315255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get network\n",
    "vgg = VGG()\n",
    "vgg.load_state_dict(torch.load(model_dir + 'vgg_conv.pth'))\n",
    "#from torchvision import transforms, models\n",
    "#vgg = models.vgg19(pretrained=True)\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "if torch.cuda.is_available():\n",
    "    vgg.cuda()\n",
    "\n",
    "lap = LaplacianNetwork()\n",
    "for param in lap.parameters():\n",
    "    param.requires_grad = False\n",
    "if torch.cuda.is_available():\n",
    "    lap.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94fea0c2-3b5c-49ef-95d1-eb13782ba0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load images, ordered as [style_image, content_image]\n",
    "img_dirs = [style_dir,content_dir]\n",
    "content_names = [f for f in os.listdir(content_dir) if os.path.isfile(os.path.join(content_dir, f))]\n",
    "style_names = [f for f in os.listdir(style_dir) if os.path.isfile(os.path.join(style_dir, f))]\n",
    "print(content_names)\n",
    "print(style_names)\n",
    "for cn in content_names:\n",
    "    for sn in style_names:\n",
    "        img_names = [sn,cn]\n",
    "        imgs = [Image.open(img_dirs[i] + name) for i,name in enumerate(img_names)]\n",
    "        imgs_torch = [prep(img) for img in imgs]\n",
    "        if torch.cuda.is_available():\n",
    "            imgs_torch = [Variable(img.unsqueeze(0).cuda()) for img in imgs_torch]\n",
    "        else:\n",
    "            imgs_torch = [Variable(img.unsqueeze(0)) for img in imgs_torch]\n",
    "        style_image, content_image = imgs_torch\n",
    "\n",
    "        opt_img = Variable(torch.randn(content_image.size()).type_as(content_image.data), requires_grad=True) #random init\n",
    "        #opt_img = Variable(content_image.data.clone(), requires_grad=True)\n",
    "        for img in imgs:\n",
    "            plt.imshow(img);plt.show()\n",
    "        #define layers, loss functions, weights and compute optimization targets\n",
    "        style_layers = ['r11','r21','r31','r41', 'r51'] \n",
    "        content_layers = ['r42']\n",
    "        loss_layers = style_layers + content_layers\n",
    "        loss_fns = [GramMSELoss()] * len(style_layers) + [nn.MSELoss()] * len(content_layers)\n",
    "        if torch.cuda.is_available():\n",
    "            loss_fns = [loss_fn.cuda() for loss_fn in loss_fns]\n",
    "    \n",
    "        #these are good weights settings:\n",
    "        style_weights = [1e3/n**2 for n in [64,128,256,512,512]]\n",
    "        content_weights = [1e0]\n",
    "        weights = style_weights + content_weights\n",
    "        \n",
    "        #compute optimization targets\n",
    "        style_targets = [GramMatrix()(A).detach() for A in vgg(style_image, style_layers)]\n",
    "        content_targets = [A.detach() for A in vgg(content_image, content_layers)]\n",
    "        targets = style_targets + content_targets\n",
    "\n",
    "        #run style transfer\n",
    "        max_iter = 400\n",
    "        show_iter = 50\n",
    "        optimizer = optim.LBFGS([opt_img],lr=1);\n",
    "        optimizer = optimizer\n",
    "        n_iter=[0]\n",
    "\n",
    "        while n_iter[0] <= max_iter:\n",
    "            torch.cuda.empty_cache()\n",
    "            def closure():\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                out = vgg(opt_img, loss_layers)\n",
    "                layer_losses = [weights[a] * loss_fns[a](A, targets[a]) for a,A in enumerate(out)]\n",
    "                deep_loss = depth_loss(opt_img,imgs[1],depth_estimator=depth_estimator)\n",
    "                loss = torch.sum(torch.stack(layer_losses)) + deep_loss \n",
    "                loss.backward()\n",
    "                n_iter[0]+=1\n",
    "                if n_iter[0]%show_iter == (show_iter-1):\n",
    "                    print('Iteration: %d, loss: %f'%(n_iter[0]+1, loss.item()))\n",
    "                return loss\n",
    "            optimizer.step(closure)\n",
    "            if n_iter[0] >= max_iter:\n",
    "                break\n",
    "        #display result\n",
    "        out_img = postp(opt_img.data[0].cpu().squeeze())\n",
    "        plt.imshow(out_img)\n",
    "        plt.show()\n",
    "        plt.imsave(\"Depth\"+cn+sn+\".png\",out_img)\n",
    "        gcf().set_size_inches(10,10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
